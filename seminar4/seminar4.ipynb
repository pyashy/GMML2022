{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "\n",
    "## Seminar 4: Nonlinear Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sklearn\n",
    "!pip install --upgrade grakel\n",
    "!pip install --upgrade POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits, fetch_olivetti_faces, make_moons, make_circles, make_swiss_roll, fetch_openml\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel, sigmoid_kernel, cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.manifold import MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    "\n",
    "from grakel import GraphKernel\n",
    "from grakel.kernels import VertexHistogram, ShortestPath, WeisfeilerLehman\n",
    "from grakel.datasets import fetch_dataset\n",
    "\n",
    "import ot\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y = make_moons(n_samples=500, noise=0.01, random_state=42)\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_std[y==0, 0], X_std[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_std[y==1, 0], X_std[y==1, 1], color='blue', alpha=0.25)\n",
    "plt.title('A nonlinear 2D dataset')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "pca_1d = PCA(n_components=1)\n",
    "X_pca_1d = pca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Kernels\n",
    "\n",
    "Kernel is any symmetric positive-definite function of the form:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{x}') \\rangle$$\n",
    "\n",
    "where $\\phi$ is a function that projects vectors $\\mathbf{x}$ and $\\mathbf{x}'$ into a new vector space. The kernel function can be seen as a function that computes the inner-product between two vectors in some, potentially infinite-dimensinal space.\n",
    "\n",
    "### 1.1.1 Linear kernel\n",
    "\n",
    "A _linear kernel_ is given by $\\mathbf{x}^T\\mathbf{x}$, thus the inner product in the original vector space. Consider a $n \\times m$ dataset $\\mathbf{X} = \\{ \\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ with $n$ observations and $m$ features, then _Gram matrix_ $\\mathbf{G} = \\mathbf{X} \\mathbf{X}^T$ is a matrix of inner-products, in this notation covariance matrix $\\mathbf{S}$ is given by $\\mathbf{X}^T\\mathbf{X}$.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Compute the matrix of pairwise linear kernels for a moons dataset $\\mathbf{X}$, check that it is Gram matrix. You can use `linear_kernel` from `sklearn.metrics.pairwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = # your code here\n",
    "G = # your code here\n",
    "\n",
    "K, G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA with linear kernel obviously is equivalent to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Kernel PCA with linear kernel\n",
    "kpca = KernelPCA(n_components=2)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1)\n",
    "X_kpca_1d = kpca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w linear kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca_1d[y==0, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_pca_1d[y==1, 0], np.zeros((int(X_pca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Gaussian kernel\n",
    "\n",
    "A radial basis function (RBF) is a real-valued function whose value depends only on the distance $d_X$ from some other point $x'$, so that $\\phi (\\mathbf {x} ,\\mathbf {x'} )= \\phi (\\|\\mathbf {x} -\\mathbf {x'} \\|)$.\n",
    "\n",
    "Many distances $d_X$ induce radial basis kernels which given by:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\exp \\left(- \\gamma \\| \\mathbf{x} -\\mathbf{x}' \\|^2\\right)$$\n",
    "\n",
    " If $\\gamma = \\frac{1}{2 \\sigma^2}$ it is known as the Gaussian kernel:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\exp \\left(- \\frac{\\| \\mathbf{x} -\\mathbf{x}' \\|^2 }{2 \\sigma^2}\\right)$$\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Compute the matrix of pairwise RBF kernels for a dataset $\\mathbf{X}$. Implement Gaussian kernel, check whether it values are equals scikit-learn solution. Check kernel values for different $\\gamma$ and $\\sigma$. You can use `rbf_kernel` from `sklearn.metrics.pairwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute K for X, matrix of paiwise RBF kernels for dataset X\n",
    "K = # your code here\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Gaussian kernel\n",
    "def gaussian_kernel(x, x_prime, sigma=1):\n",
    "    return # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply Kernel PCA with Gaussian kernel with different values of $\\gamma$ to Moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with Gaussian kernel\n",
    "gamma = 20\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=gamma)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1, kernel=\"rbf\", gamma=gamma)\n",
    "X_kpca_1d = kpca_1d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w Gaussian kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca_1d[y==0, 0], np.zeros((int(X_kpca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca_1d[y==1, 0], np.zeros((int(X_kpca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Polynomial kernel\n",
    "\n",
    "The polynomial kernel is defined as:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x'}) = (\\gamma \\mathbf{x}^\\top \\mathbf{x}' + c)^d$$\n",
    "\n",
    "where $\\mathbf{x}, \\mathbf{x'}$ are the input vectors, $\\gamma$, $c$ are scalars and $d$ is the kernel degree.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Apply KernelPCA with polynomial kernel to Moons dataset.\n",
    "\n",
    "Check how standartization affects embedding with polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with Gaussian kernel\n",
    "degree = 3\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"poly\", degree=degree)\n",
    "X_kpca = kpca.fit_transform(X_std)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1, kernel=\"poly\", degree=degree)\n",
    "X_kpca_1d = kpca_1d.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w polynomial kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca_1d[y==0, 0], np.zeros((int(X_kpca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca_1d[y==1, 0], np.zeros((int(X_kpca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4. Cosine similarity\n",
    "\n",
    "Cosine similarity is the normalized dot product between vectors $\\mathbf{x}, \\mathbf{x}'$ which is the cosine of the angle between the points denoted by the vectors:\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\frac{\\mathbf{x} \\mathbf{x}'^\\top}{\\|\\mathbf{x}\\| \\|\\mathbf{x}'\\|}$$\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Apply Kernel PCA with cosine similarity `cosine_similarity` as precomputed kernel to Moons dataset.\n",
    "\n",
    "Check how standartization affects embedding with cosine similarity kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute K, matrix of paiwise cosine similarities of dataset X\n",
    "K = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KernelPCA with cosine similarity as precomputed kernel\n",
    "kpca = KernelPCA(n_components=2, kernel=\"precomputed\")\n",
    "X_kpca = kpca.fit_transform(K)\n",
    "\n",
    "kpca_1d = KernelPCA(n_components=1, kernel=\"precomputed\")\n",
    "X_kpca_1d = kpca_1d.fit_transform(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4.5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_std[y==0, 0], X_std[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_std[y==1, 0], X_std[y==1, 1], color='blue', alpha=0.25)\n",
    "#plt.scatter(X[y==0, 0], X[y==0, 1], color='green', alpha=0.25)\n",
    "#plt.scatter(X[y==1, 0], X[y==1, 1], color='orange', alpha=0.25)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title('Kernel PCA /w cosine similarity kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', alpha=0.25)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title('Projection to PC1')\n",
    "plt.xlabel('PC1')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca_1d[y==0, 0], np.zeros((int(X_kpca.shape[0]/2),1)), color='red', alpha=0.25)\n",
    "plt.scatter(X_kpca_1d[y==1, 0], np.zeros((int(X_kpca.shape[0]/2),1)), color='blue', alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Graph kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph dataset\n",
    "mutag = fetch_dataset(\"PROTEINS\", verbose=False)\n",
    "X_mutag, y_mutag = mutag.data, mutag.target\n",
    "y_mutag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mutag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitalize Weisfeiler-Lehman kernel with Vertex histogram as the base kernel\n",
    "wl_kernel = WeisfeilerLehman(n_iter=5, normalize=True, base_graph_kernel=VertexHistogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Weisfeiler-Lehman kernel\n",
    "K = wl_kernel.fit_transform(X_mutag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Compute the Kernel PCA embedding of MUTAG graph dataset, given by Weisfeiler-Lehman kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Kernel PCA with Weisfeiler-Lehman kernel as precomputed kernel\n",
    "kpca = #your code here\n",
    "X_kpca_mutag = #your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.title('Graph embedding w/ Kernel PCA')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca_mutag[y_mutag==2, 0], X_kpca_mutag[y_mutag==2, 1], color='red', alpha=0.5)\n",
    "plt.scatter(X_kpca_mutag[y_mutag==1, 0], X_kpca_mutag[y_mutag==1, 1], color='blue', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, color = make_swiss_roll(n_samples=1000, random_state=123)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.rainbow)\n",
    "plt.title('Swiss Roll in 3D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply Kernel PCA with different kernels and kernel parameters to Swiss roll dataset. Compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize the data\n",
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=2, kernel=\"poly\", degree=9)\n",
    "X_kpca = kpca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5.75))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Original data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Z')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_std[:, 0], X_std[:, 2], c=color, cmap=\"Reds\", alpha=0.25)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Kernel PCA /w Gaussian kernel')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=color, cmap=\"Reds\", alpha=0.25)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Kernel PCA with standard kernels fails to unroll swiss roll dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manifold Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\n",
    "X_pca = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelPCA(n_components=2, kernel=\"poly\", degree=3)\n",
    "X_kpca = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=color, cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 MDS\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Embedding should preserve distances between points.\n",
    "\n",
    "$$J_{MDS}(z_1, \\dots, z_n) = \\sum_{ij}^n ((x_i, x_j) - (z_i, z_j))^2$$\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Given a dataset $X = \\{x_1, \\dots, x_n \\} \\in \\mathbb{R}^{D \\times n}$,\n",
    "\n",
    "1. Compute Gram matrix of $G \\in \\mathbb{R}^{n \\times n}$, s.t. $g_{ij} = (x_i, x_j) = X^TX$\n",
    "2. Find $d$ eigenvectors $\\{ v_1, \\dots, v_d \\} \\in \\mathbb{R}^{n \\times d}$ of $G$, corresponding to $d$ largest eigenvalues: $\\Lambda = diag(\\lambda_1, \\dots, \\lambda_d) \\in \\mathbb{R}^{d \\times d}$.\n",
    "3. Compute the embedding $\\{z_1, \\dots, z_n \\} = \\Lambda^{1/2} \\{ v_1, \\dots, v_d \\}^T$\n",
    "\n",
    "### 2.4 Isomap\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Manifold is globally isometric to a convex subset of Euclidean space.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Given a dataset $X = \\{x_1, \\dots, x_n \\} \\in \\mathbb{R}^{D \\times n}$,\n",
    "\n",
    "1. Estimate the neighborhood $\\mathcal{N}_i$ of each point, eigher within $\\varepsilon$-ball or take $k$ nearest neighbors.\n",
    "2. Build a graph on points, with adjancency matrix $A$, s.t. $a_{ij} = 1$ if the points are neighbors and $0$ otherwise.\n",
    "3. Estimate pairwise geodesic distances between any two points, by shortest paths between nodes of the graph.\n",
    "4. Perform MDS on pairwise distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Isomap(n_components=2)\n",
    "X_isomap = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=color, cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Locally Linear Embedding\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Manifold is locally linear. The same weights that reconstruct the datapoints in $D$ dimensions should reconstruct it in the manifold in $d$ dimensions.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Given a dataset $X = \\{x_1, \\dots, x_n \\} \\in \\mathbb{R}^D$,\n",
    "\n",
    "- Estimate the neighborhood $\\mathcal{N}_i$ of each point, eigher within $\\varepsilon$-ball or take $k$ nearest neighbors.\n",
    "- \n",
    "\n",
    "$$W = \\arg \\min_W \\sum_i^n \\| x_i - \\sum_j^n W_{ij} x_j \\|^2, \\\\\n",
    "s.t. \\sum_j^n W_{ij} = 1, \\forall i, \\\\\n",
    "W_{ij} = 0, x_j \\notin \\mathcal{N}_i$$\n",
    "\n",
    "- Given the weights W, find the embedded points:\n",
    "\n",
    "$$\\{z_1, \\dots, z_n\\} = \\arg \\min_{\\{z_1, \\dots, z_n\\}} \\sum_i^n \\| z_i - \\sum_i^n W_{ij} z_j \\|^2 \\\\\n",
    "s.t. \\sum_i^n z_i = 0\\\\\n",
    "cov(Z) = \\mathbb{I}$$\n",
    "\n",
    "The same weights that reconstruct the datapoints in $D$ dimensions should reconstruct it in the manifold in $d$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocallyLinearEmbedding(n_components=2, n_neighbors=15)\n",
    "X_lle = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=color, cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Laplacian Eigenmaps\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Manifold can be reconstructed with computing Laplacian of the graph, which is empirical version of Laplace-Beltrami operator of smooth manifold.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Given a dataset $X = \\{x_1, \\dots, x_n \\} \\in \\mathbb{R}^{D \\times n}$,\n",
    "\n",
    "1. Estimate the neighborhood $\\mathcal{N}_i$ of each point, eigher within $\\varepsilon$-ball or take $k$ nearest neighbors.\n",
    "2. Build adjancency graph, with adjacency matrix $A$ s.t.\n",
    "\n",
    "$$a_{ij} = \\exp(\\lambda \\| x_i - x_j \\|^2), \\textrm{if}~x_j \\in \\mathcal{N}_i \\\\\n",
    "a_{ij} = 0,~\\textrm{otherwise}$$\n",
    "\n",
    "3. Compute graph Laplacian $L = D - A \\in \\mathbb{R}^{n \\times n}$, where $D$ s.t. $d_{ii} = \\sum_i^n A_{ij}$ and solve eigenvalue problem: \n",
    "\n",
    "$$Lv = \\lambda Dv,\\\\\n",
    "v \\in \\mathbb{R}^n$$\n",
    "\n",
    "Given $d+1$ eigenvectors, corresponding to _smallest_ eigenvalues, compute the emdedding:\n",
    "\n",
    "$$\\{z_1, \\dots, z_n \\} = \\{f_1(i), \\dots, f_d(i)\\}^T \\in \\mathbb{R}^d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectralEmbedding(n_components=2, n_neighbors=25)\n",
    "X_le = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_le[:, 0], X_le[:, 1], c=color, cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Local Tangent Space Alignment\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "Local linearity assumption. Manifold can be approximated using tangent spaces to each point.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "Given a dataset $X = \\{x_1, \\dots, x_n \\} \\in \\mathbb{R}^{D \\times n}$,\n",
    "\n",
    "1. Estimate the neighborhood $\\mathcal{N}_i$ of each point, eigher within $\\varepsilon$-ball or take $k$ nearest neighbors.\n",
    "2. Build a graph on points, with adjancency matrix $A$, s.t. $a_{ij} = 1$ if the points are neighbors and $0$ otherwise.\n",
    "3. Estimate tangent spaces to each point using Local PCA, align tangent spaces.\n",
    "4. Compute the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocallyLinearEmbedding(n_components=2, n_neighbors=15, method=\"ltsa\")\n",
    "X_ltsa = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_ltsa[:, 0], X_ltsa[:, 1], c=color, cmap=plt.cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Evaluating the performance\n",
    "\n",
    "To evaluate the performance of manifold learning algorithms implement neighborhood preserving ratio (NPR) metric, which is defined as follows:\n",
    "\n",
    "$$NPR = \\frac{1}{kn} \\sum_{i=1}^n \\left|\\mathcal{N}_k(x_i) \\bigcap \\mathcal{N}_k(z_i) \\right|,$$\n",
    "\n",
    "where $n$ is the number of data points, $\\mathcal{N}_k(x_i)$ is the set of $k$-nearest neighbors of data points $x_i$ of original dimension and $\\mathcal{N}_k(z_i)$ is the set of $k$-nearest neighbors of $z_i$ of reduced dimension. $|\\cdot|$ represents the number of intersection points.\n",
    "\n",
    "**Exercise:** Evaluate manifold learning methods performance using NPR as a metric on swiss roll for 3 different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NPR(X, Z, k=21):\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPR(X, X_kpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [X_pca, X_kpca, X_isomap, X_lle, X_le, X_ltsa]\n",
    "\n",
    "k = 21\n",
    "n = X.shape[0]\n",
    "\n",
    "for z in data:\n",
    "    print(NPR(X, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "shape = (int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot numbers\n",
    "_, ax = plt.subplots(10, 10, figsize=(6, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(X[i].reshape(shape), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n components, k nearest neighbors\n",
    "n, k = 2, 15\n",
    "\n",
    "models = [\n",
    "    (\"PCA\", PCA(n_components=n)),\n",
    "    (\"Isomap\", Isomap(n_components=n, n_neighbors=k)),\n",
    "    (\"LLE\", LocallyLinearEmbedding(n_components=n, n_neighbors=k)),\n",
    "    (\"Laplacian Eignenmaps\", SpectralEmbedding(n_components=n, n_neighbors=k))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,25))\n",
    "\n",
    "for key, model in enumerate(models):\n",
    "    X_transformed = MinMaxScaler().fit_transform(model[1].fit_transform(X))\n",
    "    plt.subplot(320 + (key+1))\n",
    "    plt.title(model[0])\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    plt.scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=0.5, s=10, c=y, cmap=plt.cm.rainbow)\n",
    "    \n",
    "    # plot images\n",
    "    ax = plt.gca()\n",
    "    for i in range(0, X.shape[0], 10):\n",
    "        box = offsetbox.AnnotationBbox(offsetbox.OffsetImage(X[i].reshape(shape), cmap=\"gray\", zoom=1.5), X_transformed[i], frameon=False)\n",
    "        ax.add_artist(box)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "train_size = 60000\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "mnist.data[:train_size,:], mnist.data[train_size:,:], \\\n",
    "mnist.target[:train_size], mnist.target[train_size:]\n",
    "\n",
    "print(\"Dataset summary:\\nSamples: {}, features: {}, classes: {}\"\n",
    "      .format(X_train.shape[0] + X_test.shape[0], X_train.shape[1], np.unique(y_train).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** explore manifolds of different digits, try to interpret embedding coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test[y_test.astype(int)==8]\n",
    "shape = (int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot numbers\n",
    "_, ax = plt.subplots(10, 10, figsize=(6, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(X[i].reshape(shape), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n components, k nearest neighbors\n",
    "n, k = 2, 15\n",
    "\n",
    "models = [\n",
    "    (\"PCA\", PCA(n_components=n)),\n",
    "    (\"Isomap\", Isomap(n_components=n, n_neighbors=k)),\n",
    "    (\"LLE\", LocallyLinearEmbedding(n_components=n, n_neighbors=k)),\n",
    "    (\"Laplacian Eignenmaps\", SpectralEmbedding(n_components=n, n_neighbors=k))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,25))\n",
    "\n",
    "for key, model in enumerate(models):\n",
    "    X_transformed = MinMaxScaler().fit_transform(model[1].fit_transform(X))\n",
    "    plt.subplot(320 + (key+1))\n",
    "    plt.title(model[0])\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    #plt.scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=0.5, s=10)\n",
    "    \n",
    "    # plot images\n",
    "    ax = plt.gca()\n",
    "    for i in range(0, X.shape[0], 10):\n",
    "        box = offsetbox.AnnotationBbox(offsetbox.OffsetImage(X[i].reshape(shape), cmap=plt.cm.get_cmap('gray'), zoom=0.75), X_transformed[i], frameon=False)\n",
    "        ax.add_artist(box)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points = np.loadtxt('../seminar1/data/ref_points.csv', delimiter=',')\n",
    "X = np.loadtxt('../seminar1/data/airfoils.csv', delimiter=',')\n",
    "X_oos = np.loadtxt('../seminar1/data/test_afl.csv', delimiter=',').reshape(1, -1)\n",
    "X.shape, X_oos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n components, k nearest neighbors\n",
    "n, k = 2, 15\n",
    "\n",
    "models = [\n",
    "    (\"PCA\", PCA(n_components=n)),\n",
    "    (\"Isomap\", Isomap(n_components=n, n_neighbors=k)),\n",
    "    (\"LLE\", LocallyLinearEmbedding(n_components=n, n_neighbors=k)),\n",
    "    (\"Laplacian Eignenmaps\", SpectralEmbedding(n_components=n, n_neighbors=k))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,25))\n",
    "\n",
    "# color, proportional to height-to-width ratio\n",
    "colors = MinMaxScaler().fit_transform((np.max(X[:, :], axis=1) - np.min(X[:, :], axis=1)).reshape(-1, 1)).ravel()\n",
    "\n",
    "for key, value in enumerate(models):\n",
    "    X_transformed = MinMaxScaler().fit_transform(value[1].fit_transform(X))\n",
    "    plt.subplot(320 + (key+1))\n",
    "    plt.title(value[0])\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=colors, cmap=\"Reds\")\n",
    "    \n",
    "    for i in range(0, 199, 10):\n",
    "        plt.plot(X_transformed[i, 0] + ref_points / 4, X_transformed[i, 1] + X[i, :] / 2, '-', label = 'Airfoil #' + str(i), c='k', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Out-of-Sample extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LocallyLinearEmbedding(n_components=2, n_neighbors=21).fit(X)\n",
    "\n",
    "X_transformed = model.transform(X)\n",
    "X_transformed_oos = model.transform(X_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = MinMaxScaler().fit(X_transformed)\n",
    "X_transformed = scaler.transform(X_transformed)\n",
    "X_transformed_oos = scaler.transform(X_transformed_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=colors, cmap=\"Reds\")\n",
    "    \n",
    "for i in range(0, 199, 10):\n",
    "    plt.plot(X_transformed[i, 0] + ref_points / 4, X_transformed[i, 1] + X[i, :] / 2, '-', c='k', alpha=0.5)\n",
    "\n",
    "plt.plot(X_transformed_oos[0, 0] + ref_points / 4, X_transformed_oos[0, 1] + X_oos[0, :] / 2, '-', c='b', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Non-Euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_n = 500\n",
    "\n",
    "X = np.load(\"./data/lm_3000.npy\")\n",
    "X = X[:take_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(16.75,3))\n",
    "    \n",
    "for i in range(5):\n",
    "    ax[i].invert_yaxis()\n",
    "    ax[i].set_title(\"Point cloud {}\".format(i))\n",
    "    ax[i].scatter(X[i,:,0], X[i,:,1], c=\"b\", s=7)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $p$-Wasserstein distance\n",
    "\n",
    "Given 2 sets of vectors $\\mathbf{a} \\in \\mathbb{R}^n$ and $\\mathbf{b} \\in \\mathbb{R}^m$ (not nessesarily of same cardinality) the $p$-Wasserstein distance\n",
    "\n",
    "\\begin{equation}\n",
    "    W_p^p := \\min_{\\mathbf{P} \\in \\mathbf{U}(\\mathbf{a}, \\mathbf{b})} \\langle \\mathbf{D}, \\mathbf{P} \\rangle_F,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{U}(\\mathbf{a}, \\mathbf{b}) := \\{ P \\in \\mathbb{R}^{n \\times m}_{\\geq 0} \\mid \\mathbf{P} \\mathbf{1}_m = \\mathbf{a}, \\mathbf{P}^T \\mathbf{1}_n = \\mathbf{b} \\}$, and $\\mathbf{D}$ is matrix of $p$-th powers of the _ground distance_ between all pairs of vectors of $\\mathbf{a}$ and $\\mathbf{b}$. For $\\mathbb{R}^n$ it is the Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n, m, _ = X.shape\n",
    "    \n",
    "# distance matrix\n",
    "D = np.zeros((n, n))\n",
    "\n",
    "# uniform measures as points\n",
    "a = np.ones(m) / m\n",
    "b = np.ones(m) / m\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "\n",
    "        # squared Euclidean distance as the ground metric\n",
    "        M_ij = ot.dist(X[i], X[j], metric=\"sqeuclidean\")\n",
    "\n",
    "        # 2-Wasserstein distance, take square root\n",
    "        D[i,j] = ot.emd2(a, b, M_ij) ** 0.5\n",
    "\n",
    "D = D + D.T\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pc_mds = MDS(dissimilarity=\"precomputed\", max_iter=1000, eps=1e-6, n_jobs=-1).fit_transform(D)\n",
    "X_pc_le = Isomap(metric=\"precomputed\", n_jobs=-1).fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization purpose only\n",
    "a_mds = np.array([0.05, 0.05])\n",
    "a_isomap = np.array([0.1, 0.05])\n",
    "b_mds = np.array([-0.025, -0.027])\n",
    "b_isomap = np.array([-0.05, -0.027])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16.75,8))\n",
    "\n",
    "ax[0].invert_yaxis()\n",
    "ax[0].grid(linestyle=\"dotted\")\n",
    "ax[0].set_title(\"MDS w/ Wasserstein distance\".format(i))\n",
    "ax[0].scatter(X_pc_mds[:,0], X_pc_mds[:,1], c=\"grey\", s=2)\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].scatter(X[i,:,0] * a_mds[0] + b_mds[0] + X_pc_mds[i,0], X[i,:,1] * a_mds[1] + b_mds[1] + X_pc_mds[i,1], s=7, c=\"b\")\n",
    "    ax[0].scatter(X_pc_mds[i,0], X_pc_mds[i,1], c=\"red\", s=20)\n",
    "\n",
    "ax[1].invert_yaxis()\n",
    "ax[1].grid(linestyle=\"dotted\")\n",
    "ax[1].set_title(\"Isomap w/ Wasserstein distance\".format(i))\n",
    "ax[1].scatter(X_pc_le[:,0], X_pc_le[:,1], c=\"grey\", s=2)\n",
    "\n",
    "for i in range(10):\n",
    "    ax[1].scatter(X[i,:,0] * a_isomap[0] + b_isomap[0] + X_pc_le[i,0], X[i,:,1] * a_isomap[1] + b_isomap[1] + X_pc_le[i,1], s=7, c=\"b\")\n",
    "    ax[1].scatter(X_pc_le[i,0], X_pc_le[i,1], c=\"red\", s=20)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
