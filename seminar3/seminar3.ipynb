{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 3: Intrinsic Dimension Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "from math import pi\n",
    "from sklearn.datasets import load_digits, fetch_olivetti_faces\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximum Likelihood Estimation of Intrinsic Dimension\n",
    "\n",
    "_Levina, E., & Bickel, P. J. (2005).  \n",
    "Maximum Likelihood Estimation of Intrinsic Dimension.  \n",
    "In Advances in Neural Information Processing Systems (pp. 777-784)._\n",
    "\n",
    "The idea of the nearest-neighbor intrinsic dimension estimation is based on fact of the  that the number of sample points in $\\mathbf{X} = \\mathbb{R}^n$ falling into a ball around $\\mathbf{x}$ is proportional to the radius of of the ball $R$, the dimension of the ball $d$ and the density $f(x)$ for which the well-known estimator exists\n",
    "\n",
    "$$f(x) = \\frac{k/n}{R^d V(d)},$$\n",
    "\n",
    "where $V(d) = \\pi^{d/2} [\\Gamma (d/2+1)]^{-1}$ is the volume of unit sphere in $\\mathbb{R}^d$ and $\\Gamma(d) = (d - 1)!$ is the Gamma function.\n",
    "\n",
    "Then the intrinsic dimension $d$ of the sample can be regressed out or estimated by the maximum likelihood appoarch.\n",
    "\n",
    "Consider a dataset $\\mathbf{X} \\in \\mathbb{R}^n$ of intrinsic dimension $d$. Fix a sample point $\\mathbf{x} \\in \\mathbf{X}$. Define $B(\\mathbf{x}, t)$ as a ball centered at $\\mathbf{x}$ with radius $t$, such that $0 \\leq t \\leq R$, where $R$ is arbitrary small radius. Consider a random process, which counts the number of observations within a distance $t$ to $\\mathbf{x}$\n",
    "\n",
    "$$N(\\mathbf{x}, t) = \\sum_n \\mathbb{I} \\{\\| \\mathbf{x}_i - \\mathbf{x} \\| \\leq t \\} = \\#\\{\\mathbf{x}_i \\in B(\\mathbf{x}, t) \\}.$$\n",
    "\n",
    "Approximating this (binomial, w/ fixed $n$) random process by a Poisson process and suppressing the dependence on $\\mathbf{x}$ for now, we can write the rate $\\lambda(t)$ of the Poisson process $N(t)$ as \n",
    "\n",
    "$$\\lambda(t) = f(\\mathbf{x}) \\cdot V(d) \\cdot d \\cdot t^{d-1}.$$\n",
    "\n",
    "Let $\\theta = \\log f(x)$ then log-likelihood of the observed process is:\n",
    "\n",
    "$$L(d, \\theta) = \\int_{0}^{R} \\log \\lambda(t) d N(\\mathbf{x}, t) - \\int_{0}^{R} \\lambda(t) dt$$\n",
    "\n",
    "which should be maximized over parameters $d$ and $\\theta$.\n",
    "\n",
    "Maximum likelihood estimator for dimension $d$ at point $\\mathbf{x}$ is:\n",
    "\n",
    "$$\\hat{d}_R(\\mathbf{x}) = \\left[ \\frac{1}{N(\\mathbf{x}, R)} \\sum_{j=1}^{N(\\mathbf{x}, R)} \\log \\frac{R}{r_j(\\mathbf{x})} \\right]^{-1}$$\n",
    "\n",
    "where $r_j = \\| \\mathbf{x}_j - \\mathbf{x} \\|, j = 1, 2, \\dots$ is the distance to the $j$-th nearest neighbor.\n",
    "\n",
    "In practice, it may be more convenient to grow the number of neighbors $k$ rather than the radius of the sphere $R$. Let we use $k$ nearest neighbors $\\mathbf{x}_1, \\dots, \\mathbf{x}_n$ of a point $\\mathbf{x}$ and $R = \\mathbf{x}_k$ then the MLE estimator is:\n",
    "\n",
    "$$\\hat{d}_k(\\mathbf{x}) = \\left[ \\frac{1}{k - 1} \\sum_{j=1}^{k - 1} \\log \\frac{r_k(\\mathbf{x})}{r_j(\\mathbf{x})} \\right]^{-1}$$\n",
    "\n",
    "Averaging over $\\hat{d}_k(\\mathbf{x})$ gives the intrinsic dimension of the dataset $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of 'Maximum Likelihood Estimation of Intrinsic Dimension' by Elizaveta Levina and Peter J. Bickel\n",
    " \n",
    "how to use\n",
    "----------\n",
    " \n",
    "The goal is to estimate intrinsic dimensionality of data, the estimation of dimensionality is scale dependent\n",
    "(depending on how much you zoom into the data distribution you can find different dimesionality), so they\n",
    "propose to average it over different scales, the interval of the scales [k1, k2] are the only parameters of the algorithm.\n",
    " \n",
    "This code also provides a way to repeat the estimation with bootstrapping to estimate uncertainty.\n",
    " \n",
    "Here is one example with swiss roll :\n",
    " \n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X, _ = make_swiss_roll(1000)\n",
    " \n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             X, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=500, # nb_iter for bootstrapping\n",
    "                             verbose=1, \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)\n",
    "# the shape of intdim_k_repeated is (nb_iter, size_of_interval) where \n",
    "# nb_iter is number of bootstrap iterations (here 500) and size_of_interval\n",
    "# is (k2 - k1 + 1).\n",
    " \n",
    "# Plotting the histogram of intrinsic dimensionality estimations repeated over\n",
    "# nb_iter experiments\n",
    "plt.hist(intdim_k_repeated.mean(axis=1))\n",
    " \n",
    "\"\"\"\n",
    "# from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    " \n",
    "def intrinsic_dim_sample_wise(X, k=5):\n",
    "    neighb = NearestNeighbors(n_neighbors=k+1).fit(X)\n",
    "    dist, ind = neighb.kneighbors(X)\n",
    "    dist = dist[:, 1:]\n",
    "    dist = dist[:, 0:k]\n",
    "    assert dist.shape == (X.shape[0], k)\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k-1])\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    " \n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20):\n",
    "    X = pd.DataFrame(X).drop_duplicates().values # remove duplicates in case you use bootstrapping\n",
    "    intdim_k = []\n",
    "    for k in range(k1, k2 + 1):\n",
    "        m = intrinsic_dim_sample_wise(X, k).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    " \n",
    "def repeated(func, X, nb_iter=100, random_state=None, mode='bootstrap', **func_kw):\n",
    "    if random_state is None:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    " \n",
    "    iters = range(nb_iter) \n",
    "    for i in iters:\n",
    "        if mode == 'bootstrap':\n",
    "            Xr = X[rng.randint(0, nb_examples, size=nb_examples)]\n",
    "        elif mode == 'shuffle':\n",
    "            ind = np.arange(nb_examples)\n",
    "            rng.shuffle(ind)\n",
    "            Xr = X[ind]\n",
    "        elif mode == 'same':\n",
    "            Xr = X\n",
    "        else:\n",
    "            raise ValueError('unknown mode : {}'.format(mode))\n",
    "        results.append(func(Xr, **func_kw))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Noisy) helix curve is given by parametric equation:\n",
    "\n",
    "$$H(t) = \\big( \\sin(2 \\pi t), \\cos(2\\pi t), t\\big) + \\varepsilon \\\\\n",
    "\\varepsilon \\sim \\mathcal{N}(0, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helix(t, var=0.02):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500 # n samples\n",
    "linspace = np.linspace(-1, 1, n)\n",
    "\n",
    "data_helix = helix(linspace)\n",
    "data_helix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5,6))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(data_helix[0], data_helix[1], data_helix[2], alpha=0.95, c=((data_helix[2]+1)*255), cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Estimate sample dimensionality using repeated function and given parameters. Plot histogram, print mean values and std. It could be comfortable to make histogram in a new cell. Useful functions: repeated, matplotlib.pyplot.hist=plt.hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 10\n",
    "\n",
    "sample = data_helix\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_helix.T, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Airfoils\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Estimate intrinsic dimension of airfoils sample and compare results with PCA ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_airfoils = np.loadtxt('../seminar1/data/airfoils.csv', delimiter=',')\n",
    "data_airfoils.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 20\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_airfoils, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 MNIST\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Estimate intrinsic dimension of digits sample and compare results with PCA ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mnist = load_digits().data\n",
    "data_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 5\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_mnist, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Olivetti faces\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Estimate intrinsic dimension of faces sample and compare results with PCA ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faces = fetch_olivetti_faces(shuffle=True).data\n",
    "data_faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "nb_iter = 5\n",
    "\n",
    "intdim_k_repeated = repeated(intrinsic_dim_scale_interval, \n",
    "                             data_faces, \n",
    "                             mode='bootstrap', \n",
    "                             nb_iter=nb_iter, # nb_iter for bootstrapping \n",
    "                             k1=k1, k2=k2)\n",
    "intdim_k_repeated = np.array(intdim_k_repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.suptitle(\"Intrinsic dimension estimate via MLE\")\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.xlabel(\"Neighborhood cardinality\")\n",
    "plt.ylabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle='dotted')\n",
    "\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0), 'b')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) + np.std(intdim_k_repeated, axis=0), 'r')\n",
    "plt.plot(range(k1, k2 + 1), np.mean(intdim_k_repeated, axis=0) - np.std(intdim_k_repeated, axis=0), 'r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.hist(intdim_k_repeated.mean(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local PCA\n",
    "\n",
    "Let $\\{x_1, x_2, \\dots, x_n \\} \\subset \\mathbb{R}^D$ be a data sample and $U(x_i)$ - a neighborhood of each point, excluding $x_i$ itself, with cardinality $k$. By applying PCA locally, i.e. computing eigenvalues $\\lambda_{i,1} \\geq \\lambda_{i,2} \\geq \\dots \\lambda_{i,D}$ of each neighborhood covariance matrix: $\\mathbf{S}_i = \\frac{1}{k-1}(\\mathbf{U}(x_i) - x_i) (\\mathbf{U}(x_i) - x_i)^T$ and then averaging over eigenvalues $\\lambda_j = n^{-1} \\sum_{i=1}^n \\lambda_{i,j}$, where $j = 1, 2, \\dots, D$ one can estimate intrinsic dimension $d$ by thresholding variance explained with averaged eigenvalues.\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "1. Implement Local PCA method for intrinsic dimensionality estimation (lecture slides, pp. 25-29).  \n",
    "2. Apply the method to artificial and real data, compare results with global PCA and MLE methods for different number of nearest neighbors $k$, conclude.\n",
    "\n",
    "For neighborhood selection you may use `kneigbbors` method from scikit-learn's `NearestNeighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k of nearest neighbors to select\n",
    "k = 15\n",
    "\n",
    "X = data_helix\n",
    "\n",
    "# select neighborhood U for each point\n",
    "U = # your code here\n",
    "\n",
    "# init eigenvalues array to be further averaged\n",
    "eigenvalues = np.empty((0, X.shape[1]), np.float)\n",
    "\n",
    "# 1. for each neighborhood compute the eigenvalues\n",
    "# of SVD decompositon of local neighborhood data matrix\n",
    "# 2. add them to the eigenvalues array\n",
    "for i in range(X.shape[0]):\n",
    "    \n",
    "    # 1. your code here\n",
    "    eigenvalues_i = \n",
    "    \n",
    "    # 2. add local eigenvalues to array of eigenvalues\n",
    "    eigenvalues = np.vstack((eigenvalues, eigenvalues_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over eigenvalues for all neighborhoods\n",
    "eigenvalues_mean = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Compute explained and cumulative explained variance. Use results to estimate intrinsic dimension of data samples.\n",
    "\n",
    "$$EV_i := \\frac{\\lambda_i}{\\lambda_1 + \\dots + \\lambda_D}$$\n",
    "$$CEV_i := \\frac{\\sum_{i=1}^{d} \\lambda_i}{\\sum_{j=1}^{D} \\lambda_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = # your code here\n",
    "CEV = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EV/CEVs\n",
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.loglog(EV, \"o-\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Cumulative explained variance\")\n",
    "plt.axhline(linewidth=1, y=0.99, color='r')\n",
    "plt.axhline(linewidth=1, y=0.95, color='r')\n",
    "plt.axhline(linewidth=1, y=0.9, color='r')\n",
    "plt.axhline(linewidth=1, y=0.8, color='r')\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(CEV, \"o-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply local PCA method to estimate intrinsic dimensions of the datasets previously considered in this seminar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
