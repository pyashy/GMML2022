{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometrical Methods in Machine Learning\n",
    "## Seminar 1: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_wine, load_boston, fetch_olivetti_faces, fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model data\n",
    "\n",
    "Consider an artificial dataset which is sampled from multivariate normal distribution with covariance matrix $C = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "\n",
    "mu = np.zeros(2)\n",
    "C = np.array([[3,1],[1,2]])\n",
    "data = np.random.multivariate_normal(mu, C, size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find true PC components by covariance matrix diagonalization:\n",
    "\n",
    "$$S_{TRUE} = \\mathbf{V}\\mathbf{C}\\mathbf{V}^-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "\n",
    "print(\"True eigenvalues:\\n\", eigenvalues)\n",
    "print(\"\\nTrue eigenvectors:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find principal components using PCA from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate principal components from empirical data\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "eigenvectors_hat = pca.components_\n",
    "eigenvalues_hat = pca.explained_variance_\n",
    "\n",
    "print(\"Eigenvalues:\\n\", eigenvalues_hat)\n",
    "print(\"\\nEigenvectors:\\n\", eigenvectors_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components(data, model, scatter=True):\n",
    "    W_pca = model.components_\n",
    "    \n",
    "    if scatter:\n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "    \n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"b\", alpha=0.5, label=\"Estimated PCs\")\n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"b\", alpha=0.5)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "# plot true principal components\n",
    "plt.plot(data[:,0], (eigenvectors[0,0] / eigenvectors[0,1]) * data[:,0], color=\"r\", alpha=0.5, label=\"True PCs\")\n",
    "plt.plot(data[:,0], (eigenvectors[1,0] / eigenvectors[1,1]) * data[:,0], color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot estimated principal components\n",
    "plot_principal_components(data, pca, scatter=False)\n",
    "\n",
    "plt.title(\"Multivariate gaussian\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.25)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Find principal components manually as SVD solution of PCA problem.\n",
    "\n",
    "Estimator of covariance matrix of normally distributed variables:\n",
    "\n",
    "$$\\hat{S}_{\\mathcal{N}} = \\frac{1}{n} (\\mathbf{X} - \\mathbf{\\bar{x}})^T (\\mathbf{X} - \\mathbf{\\bar{x}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center the data\n",
    "mean = # your code here\n",
    "data_centered = # your code here\n",
    "\n",
    "# estimate data covariance matrix, X.T * X\n",
    "covariance_hat = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SVD `np.linalg.eig` of centered data matrix $\\mathbf{X}^T$:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find eigenvalues and eigenvectors of data covariance matrix via SVD\n",
    "\n",
    "# your code here\n",
    "\n",
    "print(\"Numerical mean of centered data (should be close zero):\\n\", mean)\n",
    "print(\"\\nEstimation of covariance matrix:\\n\", covariance_hat)\n",
    "print(\"\\nEigenvectors:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or eigendecomposition `np.linalg.eig` of covariance matrix $\\mathbf{X}^T\\mathbf{X}$:\n",
    "\n",
    "$$\\mathbf{X}^T\\mathbf{X} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find eigenvalues and eigenvectors of data covariance matrix via eigendecomposition\n",
    "\n",
    "# your code here\n",
    "\n",
    "print(\"Numerical mean of centered data (should be close zero):\\n\", mean)\n",
    "print(\"\\nEstimation of covariance matrix:\\n\", covariance_hat)\n",
    "print(\"\\nEigenvectors:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Compute and plot the projection onto the subspace spanned by eigenvectors.\n",
    "\n",
    "Let $\\mathbf{X}$ be $n \\times D$ centered data matrix whose columns contain the different observations. Let $\\mathbf{V}$ be $D \\times d$ matrix whose columns contain $d$ largest eigenvectors. Then the projected data $\\mathbf{Z}$ is obtained as projection to the subspace spanned by $d$ largest eigenvectors $\\mathbf{Z} = \\mathbf{X} \\mathbf{V}$.\n",
    "\n",
    "New basis of data is called eigenbasis. Principal components are axes in the new eigenbasis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute projection to eigenbasis\n",
    "proj = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.title(\"Projected data\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Reduce the dimensionality of the data by the projection onto the subspace spanned by first eigenvector. Plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the dimensionality of the data\n",
    "proj = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.title(\"Projected data\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.scatter(proj, np.zeros(proj.shape[0]), alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{Z}$ be $n \\times d$ centered data matrix with $n$ observations with $d$ features with sample mean $\\mathbf{\\bar{x}}$. Let $\\mathbf{V}$ be $D \\times d$ projection matrix whose columns contain $d$ largest eigenvectors.\n",
    "\n",
    "Then PCA reconstruction is defined:\n",
    "\n",
    "$$\\mathbf{\\tilde{X}} = \\mathbf{Z}\\mathbf{V}^T + \\mathbf{\\bar{x}}$$\n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "Reconstuct reduced dimensionality data to original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstuct the data\n",
    "data_reconstructed = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "plt.title(\"Reconstructed data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(data_reconstructed[:,0], data_reconstructed[:,1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Real data\n",
    "\n",
    "### 2.1 Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points = np.loadtxt('./data/ref_points.csv', delimiter=',') # X coordinates\n",
    "X_train = np.loadtxt('./data/airfoils.csv', delimiter=',') # 199 wings\n",
    "X_test = np.loadtxt('./data/test_afl.csv', delimiter=',') # 200th wing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.ylim(-0.23, 0.23)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "#plt.plot(ref_points, test_point, '-*', label = 'Original test')\n",
    "plt.plot(ref_points, X_train[0, :], '-*', label = 'Airfoil #1')\n",
    "plt.plot(ref_points, X_train[1, :], '-*', label = 'Airfoil #2')\n",
    "plt.plot(ref_points, X_train[2, :], '-*', label = 'Airfoil #3')\n",
    "#plt.plot(ref_points, test_point, label = 'Original')\n",
    "\n",
    "plt.title(\"Airfoils\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Airfoil #1', 'Airfoil #2', 'Airfoil #3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airfoils principal components\n",
    "\n",
    "#### Exercise  \n",
    "Plot eigenvectors for the 5 largest and 5 smallest eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "\n",
    "# your code here\n",
    "components = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    fig = plt.figure(figsize=(6,2.5))\n",
    "    plt.ylim(-0.3, 0.3)\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    plt.plot(ref_points, pca.components_[i, :], '-*')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Component #{}'.format(i+1)])\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(len(pca.components_) - 5, len(pca.components_)):\n",
    "    fig = plt.figure(figsize=(6,2.5))\n",
    "    plt.ylim(-0.3, 0.3)\n",
    "    plt.grid(linestyle=\"dotted\")\n",
    "    plt.plot(ref_points, pca.components_[i, :], '-*')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(['Component #{}'.format(i+1)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Explained variance of $i$-th principal component is the ratio of corresponding eigenvalue to the sum of all eigenvalues:\n",
    "\n",
    "$$\\mathrm{EV_i} := \\frac{\\lambda_i}{\\lambda_1 + \\dots + \\lambda_D} = \\frac{\\lambda_i}{\\sum_{j=1}^{D} \\lambda_j}$$\n",
    "\n",
    "Cumulative explained variance of the first $d$ principal components is the ratio of the sum of corresponding eigenvalues to the sum of all eigenvalues:\n",
    "\n",
    "$$\\mathrm{CEV_d} := \\frac{\\sum_{i=1}^{d} \\lambda_i}{\\sum_{j=1}^{D} \\lambda_j}$$\n",
    "\n",
    "Plot expained variance and cumulative explained variance for all $D$ principal components, choose sample dimensionality, state the rule used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EV_i(i, eigenvalues):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEV_d(d, eigenvalues):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = []\n",
    "CEV = []\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    EV.append(EV_i(i, pca.explained_variance_))\n",
    "    CEV.append(CEV_d(i, pca.explained_variance_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EV/CEVs\n",
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(EV, \"o-\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Cumulative explained variance\")\n",
    "plt.axhline(linewidth=1, y=0.99, color='r')\n",
    "plt.axhline(linewidth=1, y=0.95, color='r')\n",
    "plt.axhline(linewidth=1, y=0.9, color='r')\n",
    "plt.axhline(linewidth=1, y=0.8, color='r')\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(CEV, \"o-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Describe original airfoil vector in eigenbasis and reproject back for visualization. Try different number of basis eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "\n",
    "# your code here\n",
    "\n",
    "X_test_pcs = # your code here, should be vector of original dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.ylim(-0.23, 0.23)\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "\n",
    "#plt.plot(ref_points, test_point, '-*', label = 'Original test')\n",
    "plt.plot(ref_points, X_test, '-*', label = 'Airfoil #200')\n",
    "plt.plot(ref_points, X_test_pcs, '-*', label = 'Airfoil #200 in eigenspace')\n",
    "\n",
    "plt.title(\"Airfoils\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend(['Airfoil #200', 'Airfoil #200 in eigenspace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Measure reconstruction loss of out-of-sample airfoil `X_test` with different number of principal components in terms of 2-norm, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "norms = []\n",
    "\n",
    "for n in range(X_train.shape[1]):\n",
    "    pca = PCA(n_components=(n+1))\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    sample_mean = X_train.mean(axis=0)\n",
    "    X_test_pcs = np.dot(np.dot(X_test - sample_mean, eigenvectors.T), eigenvectors) + sample_mean\n",
    "    \n",
    "    norm = # your code here\n",
    "    norms.append(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6.5))\n",
    "plt.title(\"Reconstruction loss\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(norms)\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.ylabel(\"2-norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", data_home=\"./data/mnist\", version=1, as_frame=False)\n",
    "train_size = 60000\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "mnist.data[:train_size,:], mnist.data[train_size:,:], \\\n",
    "mnist.target[:train_size].astype(int), mnist.target[train_size:].astype(int)\n",
    "\n",
    "print(\"Dataset summary:\\nSamples: {}, features: {}, classes: {}\"\n",
    "      .format(X_train.shape[0] + X_test.shape[0], X_train.shape[1], np.unique(y_train).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot expained and cumulative explained variance, choose sample dimensionality, state the rule used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EV = # your code here\n",
    "CEV = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot EV/CEVs\n",
    "fig = plt.figure(figsize=(12,5.25))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(EV)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Cumulative explained variance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.axhline(linewidth=1, y=0.99, color='r')\n",
    "plt.axhline(linewidth=1, y=0.95, color='r')\n",
    "plt.axhline(linewidth=1, y=0.9, color='r')\n",
    "plt.axhline(linewidth=1, y=0.8, color='r')\n",
    "plt.plot(CEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the digits $0 \\dots 9$ and encode it in terms of different number of principal compoments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select digit\n",
    "X_digit = X_train[y_train==0]\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(X_digit)\n",
    "eigenvectors = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode vector in eigenbasis\n",
    "sample_mean = X_digit.mean(axis=0)\n",
    "X_digit_pcs = np.dot(np.dot(X_digit - sample_mean, eigenvectors.T), eigenvectors) + sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot numbers\n",
    "shape = (28, 28)\n",
    "_, ax = plt.subplots(10, 10, figsize=(10, 10),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(X_digit_pcs[i].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Measure reconstruction loss of chosen digit within test data `X_test` with different number of principal components in terms of mean 2-norm and PSNR metric.\n",
    "\n",
    "PSNR gives the ratio between the maximum possible power of a signal and the power of corrupting noise in decibels between image and its noisy approximation:\n",
    "\n",
    "$$\\textrm{PSNR}(x, y) := 20\\log_{10} \\left( \\frac{MAX_I}{\\sqrt{MSE}} \\right) = 20\\log_{10} \\left( \\frac{MAX_I}{\\sqrt{\\frac{1}{n} \\sum (x - y)^2}} \\right)$$\n",
    "\n",
    "where $MAX_I$ is the maximum possible pixel value of the image. In our case it is 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(x, y):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isclose(psnr(np.ones(5), np.zeros(5)), 20 * np.log10(255), rel_tol=1e-3), \"Check implementation of PSNR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = []\n",
    "psnrs = []\n",
    "\n",
    "range(2)\n",
    "\n",
    "for n in list(map(lambda x: 2 ** x, range(10))):\n",
    "    print(\"N\", n)\n",
    "    pca = PCA(n_components=(n+1))\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    eigenvectors = pca.components_\n",
    "    \n",
    "    sample_mean = X_train.mean(axis=0)\n",
    "    X_test_pcs = np.dot(np.dot(X_test - sample_mean, eigenvectors.T), eigenvectors) + sample_mean\n",
    "    \n",
    "    norm = # your code here\n",
    "    psnr_metric = # your code here\n",
    "    \n",
    "    norms.append(norm)\n",
    "    psnrs.append(psnr_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6.5))\n",
    "plt.title(\"Reconstruction loss\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(norms)\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.ylabel(\"2-norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6.5))\n",
    "plt.title(\"Reconstruction loss\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.plot(psnrs)\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.ylabel(\"PSNR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCs interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. US arrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_arrests = np.loadtxt(\"./data/usarrests.csv\", delimiter=\",\", skiprows=1, usecols=(1,2,3,4))\n",
    "targets = list(np.genfromtxt('./data/usarrests.csv', delimiter=',', dtype='str', skip_header=True)[:,0])\n",
    "y_arrests = [ x.replace('\"', '') for x in targets ]\n",
    "features_arrest = list(np.genfromtxt('./data/usarrests.csv', delimiter=',', dtype='str', skip_header=False)[0,1:])\n",
    "features_arrest = [ x.replace('\"', '') for x in features_arrest ]\n",
    "#X_arrests, y_arrests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Standartize data to zero mean and unit variance with `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standartize data to zero mean and unit variance\n",
    "X_arrests_std = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_arrests_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set eigenvectors to project data to\n",
    "pc1, pc2 = 0, 1\n",
    "eigenvector_x = pca.components_[pc1]\n",
    "eigenvector_y = pca.components_[pc2]\n",
    "\n",
    "# project data into PC space\n",
    "xs = pca.transform(X_arrests_std)[:,pc1]\n",
    "ys = pca.transform(X_arrests_std)[:,pc2]\n",
    "\n",
    "# visualize projections\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "for i in range(eigenvector_x.shape[0]):\n",
    "    plt.arrow(0, 0, eigenvector_x[i] * max(xs), eigenvector_y[i] * max(ys), color='r', width=0.0005)\n",
    "    plt.text(eigenvector_x[i] * max(xs) + 0.05, eigenvector_y[i] * max(ys), features_arrest[i], color='r')\n",
    "\n",
    "# circles project documents (ie rows from csv) as points onto PC axes\n",
    "for i in range(xs.shape[0]):\n",
    "    plt.plot(xs[i], ys[i], 'bo', alpha=0.25)\n",
    "    plt.text(xs[i] - 0.05, ys[i] + 0.1, y_arrests[i], color='b', alpha=0.25)\n",
    "\n",
    "plt.title(\"Biplot\")\n",
    "plt.xlabel(\"PC \" + str(pc1 + 1))\n",
    "plt.ylabel(\"PC \" + str(pc2 + 1))\n",
    "plt.xlim((-3.5, 3.5))\n",
    "plt.ylim((-3.5, 3.5))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "features_wine = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "# X_wine, y_wine, features_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wine_std = StandardScaler().fit_transform(X_wine)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_wine_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set eigenvectors to project data to\n",
    "pc1, pc2 = 0, 1\n",
    "eigenvector_x = pca.components_[pc1]\n",
    "eigenvector_y = pca.components_[pc2]\n",
    "\n",
    "# project data into PC space\n",
    "xs = pca.transform(X_wine_std)[:,pc1]\n",
    "ys = pca.transform(X_wine_std)[:,pc2]\n",
    "\n",
    "# visualize projections\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "for i in range(eigenvector_x.shape[0]):\n",
    "    plt.arrow(0, 0, eigenvector_x[i] * max(xs), eigenvector_y[i] * max(ys), color='r', width=0.0005, alpha=0.5)\n",
    "    plt.text(eigenvector_x[i] * max(xs) + 0.05, eigenvector_y[i] * max(ys), features_wine[i], color='r')\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "# circles project documents (ie rows from csv) as points onto PC axes\n",
    "for i in range(xs.shape[0]):\n",
    "    plt.plot(xs[i], ys[i], 'o', color=colors[y_wine[i]], alpha=0.25)\n",
    "\n",
    "plt.title(\"Biplot\")\n",
    "plt.xlabel(\"PC \" + str(pc1 + 1))\n",
    "plt.ylabel(\"PC \" + str(pc2 + 1))\n",
    "plt.xlim((-4.5, 4.5))\n",
    "plt.ylim((-4.5, 4.5))\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA for decision making\n",
    "\n",
    "- high-dimensional space is more dangerous to overfit complex models, than simple ones\n",
    "- PC are orthogonal, decorellated features\n",
    "    - may improve linear classifiers, if data multicolinearity problem\n",
    "    - for complex decision boundaries classifiers, they can be a problem in even lower dimensions\n",
    "- variable scaling, ensure your data is scaled to isotropic Gaussian $\\sim \\mathcal{N}(0, 1)$ or at least have similar scale (`MinMaxScaler`)\n",
    "- you can estimate first $q$ PCs only, using iterative optimization, instead full eigenvalue/SVD decoposition and then taking top PC corresponing to largest eigenvalues\n",
    "\n",
    "### 4.1. Eigenfaces\n",
    "\n",
    "Eigenfaces as principal compoments and logistic regression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Olivetti faces dataset\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=1)\n",
    "train_size = 350\n",
    "X_train, X_test, y_train, y_test = dataset.data[:train_size,:], dataset.data[train_size:,:], dataset.target[:train_size], dataset.target[train_size:]\n",
    "shape = (64, 64)\n",
    "print(\"Dataset summary:\\nSamples: {}, features: {}, classes: {}\".format(dataset.data.shape[0], dataset.data.shape[1], np.unique(dataset.target).shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot faces\n",
    "_, ax = plt.subplots(10, 10, figsize=(10, 10),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(X_train[i].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Plot first 64 eigenfaces, comment the difference between the first and the last ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 64 components\n",
    "_, ax = plt.subplots(8, 8, figsize=(10, 10),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(pca.components_[i].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Encode any face in test dataset in eigenbasis of different powers, `n_components`= [2, 4, 8 16, 32, 64, $\\dots$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA\n",
    "face_id = 1\n",
    "pca = PCA(n_components=64)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "eigenvectors = pca.components_\n",
    "\n",
    "sample_mean = X_train.mean(axis=0)\n",
    "X_test_pcs = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_, ax = plt.subplots(1, 2, figsize=(5, 5),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "ax[0].imshow(X_test[face_id].reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")\n",
    "ax[1].imshow(X_test_pcs.reshape(shape), cmap=plt.cm.gray, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "Classify faces in original dimension with kNN, logistic regression and random forest, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "clf = # your code here\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc_full_train = accuracy_score(clf.predict(X_train), y_train)\n",
    "acc_full_test = accuracy_score(clf.predict(X_test), y_test)\n",
    "\n",
    "acc_full_train, acc_full_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Perform classification in the space of reduced dimension of your choice, conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = # your code here\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "clf = # your code here\n",
    "clf.fit(X_train_pca, y_train)\n",
    "accuracy_score(clf.predict(X_train_pca), y_train), accuracy_score(clf.predict(X_test_pca), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "- Perform classification for a whole range of dimensions, with a classifier of your choice.\n",
    "- State the maximum number of principal components for this space, conclude. \n",
    "- Plot the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_train = []\n",
    "acc_test = []\n",
    "\n",
    "for n in # your code here:\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.title(\"Classifier performance\")\n",
    "plt.xlabel(\"# PCs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(linestyle=\"dotted\")\n",
    "plt.axhline(linewidth=1, y=acc_full_test, color='r')\n",
    "plt.plot(acc_train, 'b')\n",
    "plt.plot(acc_test, 'g')\n",
    "plt.legend(['Best on test in original dimension', 'Train in reduced dimension', 'Test in reducted dimension'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
